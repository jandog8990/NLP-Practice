{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.493, 'neu': 0.507, 'pos': 0.0, 'compound': -0.5983}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Pre-trained sentiment analyzer\n",
    "# Built in pretrained analyzer - VADER (Valence Aware Dictionary and sEntiment Reasoner)\n",
    "# VADER best used for social media and short sentences\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# polarity analyzes the pos/neg words in a text\n",
    "sia.polarity_scores(\"NLTK is the fkn shit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "\n",
      "\n",
      "> True @CFindlaySnp15 if labour propose a queens speech it's up to the SNP to support it or not\n",
      "> True @GzaDaRambler Gotcha :)\n",
      "> True @Mburu__ Inter 3 UCL, Arsenal... Small team, Right! :)\n",
      "> True RT @Tommy_Colc: Financial Times come out in support of Tories claiming Miliband is \"preoccupied w/ inequality\". The man who wrote it http:/â€¦\n",
      "> True Turned the debate off after Ed Miliband because I don't give a fuck what Nick Clegg has to say #bbcqt\n",
      "> False RT @thomasmessenger: For all Tories claiming that Labour overspent and thus caused a global financial crisis, ahem... http//t.co/DkLwCwzhDA\n",
      "> False @Kitchmo the SNP always get annoyed when questioned: they live in a parallel universe. Anyway don't tell me you don't want t to vote Blair\n",
      "> False @KrystalHosting Was just about to push a client your way for some hosting. Maybe I had better wait till next week :(\n",
      "> False Ed Miliband's ignorant refusal to talk about post-election deals is playing a very dangerous game, putting himself before the country.\n",
      "> True Nigel Farage is the ultimate 'I'm not racist I have black friends' bloke\n"
     ]
    }
   ],
   "source": [
    "# twitter samples \n",
    "import nltk\n",
    "\n",
    "# get list of raw tweets with strings\n",
    "tweets = [t.replace(\"://\", \"//\") for t in nltk.corpus.twitter_samples.strings()]\n",
    "print(len(tweets))\n",
    "#tweets[:10]\n",
    "\n",
    "# popularity scores for the tweets\n",
    "from random import shuffle\n",
    "\n",
    "def is_positive(tweet: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise\"\"\"\n",
    "    return sia.polarity_scores(tweet)[\"compound\"] > 0\n",
    "\n",
    "print(\"\\n\")\n",
    "shuffle(tweets)\n",
    "for tweet in tweets[:10]:\n",
    "    print(\">\", is_positive(tweet), tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos/cv000_29590.txt\n"
     ]
    }
   ],
   "source": [
    "# positive/negativie movie reviews - already been classified using VADER\n",
    "positive_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "negative_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "all_review_ids = positive_review_ids + negative_review_ids\n",
    "print(positive_review_ids[0])\n",
    "\n",
    "# set up VADER to rate individual sentences rather than full reviews\n",
    "# VADER needs raw strings for its rating => you can't use words()\n",
    "# make a list of file IDs that corpus uses to reference individual reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.95% correct\n"
     ]
    }
   ],
   "source": [
    "# redefine is positive to work on an entire review. Obtain specific review\n",
    "# using its file ID and then split it into sentences before rating:\n",
    "from statistics import mean\n",
    "\n",
    "def is_positive(review_id: str) -> bool:\n",
    "    \"\"\"\n",
    "    True if the average of all sentence compound scores is positive\n",
    "    \"\"\"\n",
    "    \n",
    "    # first get the raw text for the entire movie\n",
    "    text = nltk.corpus.movie_reviews.raw(review_id)\n",
    "    scores = [\n",
    "        (sia.polarity_scores(sentence)[\"neg\"] and sia.polarity_scores(sentence)[\"compound\"])\n",
    "        # loop through sentences from tokenization\n",
    "        for sentence in nltk.sent_tokenize(text)\n",
    "    ]\n",
    "    return mean(scores) > 0\n",
    "\n",
    "# shuffle the review ids and find if they are positive\n",
    "shuffle(all_review_ids)\n",
    "correct = 0\n",
    "for review_id in all_review_ids:\n",
    "    if is_positive(review_id):\n",
    "        if review_id in positive_review_ids:\n",
    "            correct += 1\n",
    "    else:\n",
    "        if review_id in negative_review_ids:\n",
    "            correct += 1\n",
    "            \n",
    "print(F\"{correct / len(all_review_ids):.2%} correct\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completely\n",
      "pretty\n"
     ]
    }
   ],
   "source": [
    "# Customizing NLTK's Sentiment Analysis\n",
    "# TRICK - figure out which properties of your dataset are \n",
    "# useful in classifying each piece of data into desired categories\n",
    "\n",
    "# ML - these properties are known as features, which you select.\n",
    "\n",
    "# Selecting useful features - by using predefined categ\n",
    "# in movie_reviews corpus you can create positive/negative words.\n",
    "\n",
    "# Determine which ones occur most frequently across each set.\n",
    "# Begin excluding unwanted words and building initial category groups\n",
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "# corpus names/words\n",
    "corpus_words = nltk.corpus.names.words()\n",
    "\n",
    "# skip unwanted words that don't have letters and are not in the unwated list\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word in unwanted:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# get the POS (Part of Speech) tags from the moview reviews\n",
    "pos_reviews = nltk.corpus.movie_reviews.words(categories=[\"pos\"])\n",
    "neg_reviews = nltk.corpus.movie_reviews.words(categories=[\"neg\"])\n",
    "pos_tags = nltk.pos_tag(pos_reviews)\n",
    "neg_tags = nltk.pos_tag(neg_reviews)\n",
    "\n",
    "# filter out the positive tuples using the skip unwanted func\n",
    "pos_words = [word for word, tag in filter(skip_unwanted, pos_tags)]\n",
    "neg_words = [word for word, tag in filter(skip_unwanted, neg_tags)]\n",
    "print(pos_words[123])\n",
    "print(neg_words[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive fd len before = 15449\n",
      "9511\n",
      "immaculate\n"
     ]
    }
   ],
   "source": [
    "# create frequency distributions for custom feature\n",
    "# begin by finding the the common set of words to remove from distribution\n",
    "positive_fd = nltk.FreqDist(pos_words)\n",
    "negative_fd = nltk.FreqDist(neg_words)\n",
    "print(\"positive fd len before = \" + str(len(positive_fd)))\n",
    "\n",
    "# find the intersection between pos and neg words\n",
    "common_set = set(positive_fd).intersection(negative_fd)\n",
    "print(len(common_set))\n",
    "print(list(common_set)[0])\n",
    "\n",
    "# delete the common words in the lists\n",
    "for word in common_set:\n",
    "    del positive_fd[word]\n",
    "    del negative_fd[word]\n",
    "\n",
    "# unique pos/neg words in each freq dist, the amount of words in each set\n",
    "# can be tweaked in order to determine effect on sentiment analysis\n",
    "top_100_positive = {word for word, count in positive_fd.most_common(100)}\n",
    "top_100_negative = {word for word, count in negative_fd.most_common(100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<nltk.collocations.BigramCollocationFinder object at 0x7f46c2196e50>\n"
     ]
    }
   ],
   "source": [
    "# example of a feature to extract from data, words that aren't neg/pos\n",
    "# leverage collocations that carry positive meaning bigram finders (Eg \"thumbs up!\")\n",
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "positive_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"pos\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])\n",
    "\n",
    "negative_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"neg\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])\n",
    "\n",
    "print(positive_bigram_finder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and using a classifier - define a func to extract features from data\n",
    "# focus on features with positivity including VADER scores\n",
    "def extract_features(text):\n",
    "    features = dict()\n",
    "    wordcount = 0\n",
    "    compound_scores = list()\n",
    "    positive_scores = list()\n",
    "    \n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word.lower() in top_100_positive:\n",
    "                wordcount += 1\n",
    "        compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
    "        positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "        \n",
    "    # Adding 1 to the final compound score to always have pos numbers\n",
    "    # since some classifiers you'll use later don't work with neg numbers\n",
    "    features[\"mean_compound\"] = mean(compound_scores) + 1\n",
    "    features[\"mean_positive\"] = mean(positive_scores)\n",
    "    features[\"wordcount\"] = wordcount\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features type 0:\n",
      "({'mean_compound': 1.0785804347826087, 'mean_positive': 0.09447826086956522, 'wordcount': 2}, 'pos')\n"
     ]
    }
   ],
   "source": [
    "# extract_features returns a dictionary - 3 features for each text\n",
    "\"\"\"\n",
    "1. The average compound score\n",
    "2. The average postive score\n",
    "3. Amount of words in the text that are part of top 100 positive reviews\n",
    "\"\"\"\n",
    "\n",
    "# to train/evaluate a classifier, you'll need to build a list \n",
    "# of features for each text you'll analyze\n",
    "features = [\n",
    "    # create tuple of the dictionary and review type\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "features.extend([\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])\n",
    "\n",
    "print(\"Features type 0:\")\n",
    "print(features[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size = 1500\n",
      "Test set size = 500\n",
      "Most Informative Features\n",
      "               wordcount = 3                 pos : neg    =     13.8 : 1.0\n",
      "               wordcount = 5                 pos : neg    =     11.0 : 1.0\n",
      "               wordcount = 2                 pos : neg    =      3.8 : 1.0\n",
      "               wordcount = 4                 pos : neg    =      1.8 : 1.0\n",
      "               wordcount = 0                 neg : pos    =      1.7 : 1.0\n",
      "               wordcount = 1                 pos : neg    =      1.6 : 1.0\n",
      "           mean_positive = 0.07608333333333334    neg : pos    =      1.0 : 1.0\n",
      "           mean_positive = 0.08418181818181818    neg : pos    =      1.0 : 1.0\n",
      "           mean_positive = 0.08611764705882353    neg : pos    =      1.0 : 1.0\n",
      "           mean_positive = 0.10004761904761905    neg : pos    =      1.0 : 1.0\n",
      "\n",
      "\n",
      "({'mean_compound': 0.7512529411764706, 'mean_positive': 0.06364705882352942, 'wordcount': 0}, 'neg')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training classifier means splitting into train/eval\n",
    "# use 1/4 of the set for training\n",
    "# train_count = len(features) // 4\n",
    "from sklearn import model_selection\n",
    "train_set, test_set = model_selection.train_test_split(features, test_size = 0.25)\n",
    "print(\"Train set size = %d\" % len(train_set))\n",
    "print(\"Test set size = %d\" % len(test_set))\n",
    "\n",
    "shuffle(train_set)\n",
    "# classifier = nltk.NaiveBayesClassifier.train(features[:train_count])\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "classifier.show_most_informative_features(10)\n",
    "nltk.classify.accuracy(classifier, test_set)\n",
    "\n",
    "# try classifying a piece of the test set to see if it works\n",
    "test_tuple = test_set[0]\n",
    "test_set = test_tuple[0]\n",
    "print(\"\\n\")\n",
    "print(test_tuple)\n",
    "classifier.classify(test_tuple[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_compound': 1.0431027027027027, 'mean_positive': 0.12127027027027026, 'wordcount': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new test features set using a random field in the movie reviews\n",
    "# print \"Naive Bayes Accuracy \" + str(nltk.classify.accuracy(classifier, test_set)*100)\n",
    "testFieldId = nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])[599]\n",
    "testRaw = nltk.corpus.movie_reviews.raw(testFieldId)\n",
    "testRawFeatures = extract_features(testRaw)\n",
    "# classifier.classify(testRawTuple)\n",
    "print(testRawFeatures)\n",
    "classifier.classify(testRawFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
